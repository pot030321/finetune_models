2025-08-13 08:17:11,735 - INFO - Loading model and tokenizer from Qwen/Qwen2.5-0.5B...
2025-08-13 08:17:13,057 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-08-13 08:17:14,045 - INFO - Model and tokenizer loaded successfully!
2025-08-13 08:17:14,436 - INFO - Trainable params: 35,192,832 | Total params: 529,225,600 | Percentage: 6.65%
2025-08-13 08:17:14,611 - INFO - Loaded 22026 examples from training_data.json
2025-08-13 08:17:14,665 - INFO - Augmented from 22026 to 64786 examples
2025-08-13 08:17:14,665 - INFO - After augmentation: 64786 examples
2025-08-13 08:17:48,008 - INFO - Dataset statistics: {'total_examples': 64786, 'q2a_count': 64450, 'a2a_count': 336, 'avg_token_length': np.float64(490.7772049516871), 'max_token_length': 737, 'min_token_length': 166}
2025-08-13 08:17:49,711 - INFO - Created 64786 examples after processing
2025-08-13 08:19:04,370 - INFO - Tokenized dataset size: 64786
2025-08-13 08:19:04,379 - INFO - Train size: 55068, Eval size: 9718
2025-08-13 08:19:04,532 - INFO - Starting fine-tuning...
